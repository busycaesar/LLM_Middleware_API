# LLM Middleware APIs

## Description

This project provides an API for interacting with the **Gemini-1.5-flash** language model (LLM). It offers a streamlined interface for developers to prompt and receive responses from this powerful AI model. By blackboxing the complexities of LLM interactions, developers can focus on building applications without the overhead of directly managing LLM connections and API calls. The API can be configured by pulling the image from the DockerHub and passing the Gemini's API key as environment variable. Future plans include expanding the API to support a wider range of LLMs, offering developers more choices and customization options.

<!--
### Video Explanation

[![Video Explanation YouTube Link](https://img.youtube.com/vi/t93d8ieZn0Q/0.jpg)](https://www.youtube.com/embed/t93d8ieZn0Q)
-->

## Tech Stack

![Image Alt](https://skillicons.dev/icons?i=nodejs,express,docker)

## [Project Documentation](./documentation.md)

## [API Documentation](./api-documentation.md)

## Author

[Dev Shah](https://github.com/busycaesar)
